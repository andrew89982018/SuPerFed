{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d882d4-5ec1-40cb-8f28-23622c105039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33cae38-7d48-4c5a-83f7-958649525701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52b7b04-7c5f-4a6a-ba9b-86abca937cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, indices=None, train=True, transform=None, target_transform=None, download=False):\n",
    "        self.root = args.data_path\n",
    "        self.dataset_name = args.dataset\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "        self.indices = indices\n",
    "        self.inputs, self.targets = self._get_data()\n",
    "\n",
    "    def _get_data(self):\n",
    "        # retrieve data\n",
    "        raw = torchvision.datasets.__dict__[self.dataset_name](root=self.root, train=self.train, transform=self.transform, target_transform=self.target_transform, download=self.download)\n",
    "        \n",
    "        # get inputs and targets\n",
    "        inputs, targets = raw.data, raw.targets\n",
    "        if self.indices is not None:\n",
    "            inputs, targets = inputs[self.indices], targets[self.indices]\n",
    "        return inputs, targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError('[ERROR] Subclass should implement this!')\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError('[ERROR] Subclass should implement this!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3738c57-8c26-4a0a-8f88-441a5e5f2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(VisionDataset):\n",
    "    def __getitem__(self, index):\n",
    "        # get corresponding inputs & targets pair\n",
    "        inputs, targets = self.inputs[index], self.targets[index]\n",
    "        inputs = PIL.Image.fromarray(inputs.numpy(), mode='L')\n",
    "        \n",
    "        # apply transformation\n",
    "        if self.transform is not None:\n",
    "            inputs = self.transform(inputs)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            targets = self.target_transform(targets)\n",
    "        return inputs, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259bafd6-d8c8-43eb-a68e-cb26b5a6f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({'data_path': './data', 'dataset': 'MNIST'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0924b3-c795-43a6-9a78-e5b476efcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNISTDataset(args, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2129b0cd-4803-44bd-8277-4177e2bd5421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26be4aa3-95cc-42ad-850e-03242eb8b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMNISTDataset(VisionDataset):\n",
    "    def __init__(self, split='byclass', **kwargs):\n",
    "        self.split = split\n",
    "        super(EMNISTDataset, self).__init__(**kwargs)     \n",
    "\n",
    "    def _get_data(self):\n",
    "        # retrieve data\n",
    "        raw = torchvision.datasets.__dict__[self.dataset_name](root=self.root, train=self.train, split=self.split, transform=self.transform, target_transform=self.target_transform, download=self.download)\n",
    "        \n",
    "        # get inputs and targets\n",
    "        inputs, targets = raw.data, raw.targets\n",
    "        if self.indices is not None:\n",
    "            inputs, targets = inputs[self.indices], targets[self.indices]\n",
    "        return inputs, targets\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get corresponding inputs & targets pair\n",
    "        inputs, targets = self.inputs[index], self.targets[index]\n",
    "        inputs = PIL.Image.fromarray(inputs.numpy(), mode='L')\n",
    "        \n",
    "        # apply transformation\n",
    "        if self.transform is not None:\n",
    "            inputs = self.transform(inputs)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            targets = self.target_transform(targets)\n",
    "        return inputs, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e64c12-6ba4-4545-948b-17cd68a3ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({'data_path': './data', 'dataset': 'EMNIST'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5275036-2125-4853-b521-725be2780e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist = EMNISTDataset(args=args, download=True, transform=torchvision.transforms.ToTensor(), split='byclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2981d0c6-0a6e-4981-88b1-6113aaa0ec00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0b1da7-be7e-4905-a3c8-d8f56a8f92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARDataset(VisionDataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CIFARDataset, self).__init__(**kwargs)     \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # get corresponding inputs & targets pair\n",
    "        inputs, targets = self.inputs[index], self.targets[index]\n",
    "        inputs = PIL.Image.fromarray(inputs, mode='RGB')\n",
    "        \n",
    "        # apply transformation\n",
    "        if self.transform is not None:\n",
    "            inputs = self.transform(inputs)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            targets = self.target_transform(targets)\n",
    "        return inputs, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24c3c818-8b5d-4338-ae98-e0b46f4ac725",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({'data_path': './data', 'dataset': 'CIFAR100'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fc317e6-d482-444c-82c3-ef2b4ab4575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10 = CIFARDataset(args=args, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ae84a1-e17f-434e-965d-920dc2ed7e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b94bd5f4-09d1-4d72-b479-3b0c5bea7fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7c1235cd-b7fe-4fbc-8849-2da0af8c1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyCIFARDataset(CIFARDataset):\n",
    "    def __init__(self, args, noise_rate, **kwargs):\n",
    "        super(CIFARDataset, self).__init__(args, **kwargs)     \n",
    "        self.noise_rate = noise_rate\n",
    "        #idx_each_class_noisy = [[] for i in range(args.num_classes)]\n",
    "        \n",
    "        if self.train:\n",
    "            self.noisy_targets, self.actual_noise_rate = self.multiclass_symmetric_noisify(targets=self.targets, noise_rate=self.noise_rate, seed=args.global_seed, num_classes=args.num_classes)\n",
    "            self.noise_mask = np.transpose(self.noisy_targets) != np.transpose(self.targets)\n",
    "            #for i in range(len(self.targets)):\n",
    "            #    idx_each_class_noisy[self.noisy_labels[i]].append(i)\n",
    "            #print(idx_each_class_noisy)\n",
    "            #class_size_noisy = [len(idx_each_class_noisy[i]) for i in range(args.num_classes)]\n",
    "            #self.noise_prior = np.array(class_size_noisy) / sum(class_size_noisy)\n",
    "        self.targets, self.original_targets = self.noisy_targets, self.targets\n",
    "\n",
    "    def multiclass_noisify(self, targets, P, seed):\n",
    "        assert P.shape[0] == P.shape[1]\n",
    "        assert np.max(targets) < P.shape[0]\n",
    "        np.testing.assert_array_almost_equal(P.sum(axis=1), np.ones(P.shape[1]))\n",
    "        assert (P >= 0.0).all()\n",
    "        \n",
    "        noisy_targets = targets.copy()\n",
    "        flipper = np.random.RandomState(seed)\n",
    "\n",
    "        for idx in np.arange(len(targets)):\n",
    "            i = targets[idx]\n",
    "            flipped = flipper.multinomial(n=1, pvals=P[i, :], size=1)[0]\n",
    "            noisy_targets[idx] = np.where(flipped == 1)[0]\n",
    "        return noisy_targets\n",
    "\n",
    "    def multiclass_symmetric_noisify(self, targets, noise_rate, seed, num_classes):\n",
    "        P = np.ones((num_classes, num_classes))\n",
    "        P *= (noise_rate / (num_classes - 1)) \n",
    "\n",
    "        if noise_rate > 0.0:\n",
    "            # 0 -> 1\n",
    "            P[0, 0] = 1. - noise_rate\n",
    "            for i in range(1, num_classes-1):\n",
    "                P[i, i] = 1. - noise_rate\n",
    "            P[num_classes - 1, num_classes - 1] = 1. - noise_rate\n",
    "\n",
    "            noisy_targets = self.multiclass_noisify(targets, P, seed)\n",
    "            actual_noise = (np.array(noisy_targets).flatten() != np.array(targets).flatten()).mean()\n",
    "            assert actual_noise > 0.0\n",
    "        return np.array(noisy_targets).flatten(), actual_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "46b2bab9-f1d2-4570-9d14-29a2516cc553",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({'data_path': './data', 'dataset': 'CIFAR100', 'num_classes': 100, 'global_seed': 5959})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bc1cda61-a979-479e-b669-32c82177f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisycifar = NoisyCIFARDataset(args, noise_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1e5a3dcd-d821-4003-a9a2-807b4f03dc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F3F25C92590>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisycifar[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "27b89b4f-550a-4b36-a89a-d425737a73e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisycifar[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "503fab85-5a5d-4283-bb0f-c78c4e2cdd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100 = torchvision.datasets.__dict__['CIFAR100'](root=args.data_path, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3ed3e9fe-0f0b-4a5e-98af-f6e4dc0032b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0,\n",
       " 'aquarium_fish': 1,\n",
       " 'baby': 2,\n",
       " 'bear': 3,\n",
       " 'beaver': 4,\n",
       " 'bed': 5,\n",
       " 'bee': 6,\n",
       " 'beetle': 7,\n",
       " 'bicycle': 8,\n",
       " 'bottle': 9,\n",
       " 'bowl': 10,\n",
       " 'boy': 11,\n",
       " 'bridge': 12,\n",
       " 'bus': 13,\n",
       " 'butterfly': 14,\n",
       " 'camel': 15,\n",
       " 'can': 16,\n",
       " 'castle': 17,\n",
       " 'caterpillar': 18,\n",
       " 'cattle': 19,\n",
       " 'chair': 20,\n",
       " 'chimpanzee': 21,\n",
       " 'clock': 22,\n",
       " 'cloud': 23,\n",
       " 'cockroach': 24,\n",
       " 'couch': 25,\n",
       " 'crab': 26,\n",
       " 'crocodile': 27,\n",
       " 'cup': 28,\n",
       " 'dinosaur': 29,\n",
       " 'dolphin': 30,\n",
       " 'elephant': 31,\n",
       " 'flatfish': 32,\n",
       " 'forest': 33,\n",
       " 'fox': 34,\n",
       " 'girl': 35,\n",
       " 'hamster': 36,\n",
       " 'house': 37,\n",
       " 'kangaroo': 38,\n",
       " 'keyboard': 39,\n",
       " 'lamp': 40,\n",
       " 'lawn_mower': 41,\n",
       " 'leopard': 42,\n",
       " 'lion': 43,\n",
       " 'lizard': 44,\n",
       " 'lobster': 45,\n",
       " 'man': 46,\n",
       " 'maple_tree': 47,\n",
       " 'motorcycle': 48,\n",
       " 'mountain': 49,\n",
       " 'mouse': 50,\n",
       " 'mushroom': 51,\n",
       " 'oak_tree': 52,\n",
       " 'orange': 53,\n",
       " 'orchid': 54,\n",
       " 'otter': 55,\n",
       " 'palm_tree': 56,\n",
       " 'pear': 57,\n",
       " 'pickup_truck': 58,\n",
       " 'pine_tree': 59,\n",
       " 'plain': 60,\n",
       " 'plate': 61,\n",
       " 'poppy': 62,\n",
       " 'porcupine': 63,\n",
       " 'possum': 64,\n",
       " 'rabbit': 65,\n",
       " 'raccoon': 66,\n",
       " 'ray': 67,\n",
       " 'road': 68,\n",
       " 'rocket': 69,\n",
       " 'rose': 70,\n",
       " 'sea': 71,\n",
       " 'seal': 72,\n",
       " 'shark': 73,\n",
       " 'shrew': 74,\n",
       " 'skunk': 75,\n",
       " 'skyscraper': 76,\n",
       " 'snail': 77,\n",
       " 'snake': 78,\n",
       " 'spider': 79,\n",
       " 'squirrel': 80,\n",
       " 'streetcar': 81,\n",
       " 'sunflower': 82,\n",
       " 'sweet_pepper': 83,\n",
       " 'table': 84,\n",
       " 'tank': 85,\n",
       " 'telephone': 86,\n",
       " 'television': 87,\n",
       " 'tiger': 88,\n",
       " 'tractor': 89,\n",
       " 'train': 90,\n",
       " 'trout': 91,\n",
       " 'tulip': 92,\n",
       " 'turtle': 93,\n",
       " 'wardrobe': 94,\n",
       " 'whale': 95,\n",
       " 'willow_tree': 96,\n",
       " 'wolf': 97,\n",
       " 'woman': 98,\n",
       " 'worm': 99}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar100.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf708fe-4a82-4d0a-ad4e-7f5fd7e3c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyCIFARDataset(CIFARDataset):\n",
    "    def __init__(self, args, noise_rate=0.2, **kwargs):\n",
    "        super(CIFARDataset, self).__init__(args, **kwargs)     \n",
    "        self.noise_rate = noise_rate\n",
    "        #idx_each_class_noisy = [[] for i in range(args.num_classes)]\n",
    "        \n",
    "        if self.train:\n",
    "            self.noisy_targets, self.actual_noise_rate = self.multiclass_symmetric_noisify(targets=self.targets, noise_rate=self.noise_rate, seed=args.global_seed, num_classes=args.num_classes)\n",
    "            self.noise_mask = np.transpose(self.noisy_targets) != np.transpose(self.targets)\n",
    "            #for i in range(len(self.targets)):\n",
    "            #    idx_each_class_noisy[self.noisy_labels[i]].append(i)\n",
    "            #print(idx_each_class_noisy)\n",
    "            #class_size_noisy = [len(idx_each_class_noisy[i]) for i in range(args.num_classes)]\n",
    "            #self.noise_prior = np.array(class_size_noisy) / sum(class_size_noisy)\n",
    "        self.targets, self.original_targets = self.noisy_targets, self.targets\n",
    "\n",
    "    def multiclass_noisify(self, targets, P, seed):\n",
    "        assert P.shape[0] == P.shape[1]\n",
    "        assert np.max(targets) < P.shape[0]\n",
    "        np.testing.assert_array_almost_equal(P.sum(axis=1), np.ones(P.shape[1]))\n",
    "        assert (P >= 0.0).all()\n",
    "        \n",
    "        noisy_targets = targets.copy()\n",
    "        flipper = np.random.RandomState(seed)\n",
    "\n",
    "        for idx in np.arange(len(targets)):\n",
    "            i = targets[idx]\n",
    "            flipped = flipper.multinomial(n=1, pvals=P[i, :], size=1)[0]\n",
    "            noisy_targets[idx] = np.where(flipped == 1)[0]\n",
    "        return noisy_targets\n",
    "\n",
    "    def multiclass_symmetric_noisify(self, targets, noise_rate, seed, num_classes):\n",
    "        P = np.ones((num_classes, num_classes))\n",
    "        P *= (noise_rate / (num_classes - 1)) \n",
    "\n",
    "        if noise_rate > 0.0:\n",
    "            # 0 -> 1\n",
    "            P[0, 0] = 1. - noise_rate\n",
    "            for i in range(1, num_classes-1):\n",
    "                P[i, i] = 1. - noise_rate\n",
    "            P[num_classes - 1, num_classes - 1] = 1. - noise_rate\n",
    "\n",
    "            noisy_targets = self.multiclass_noisify(targets, P, seed)\n",
    "            actual_noise = (np.array(noisy_targets).flatten() != np.array(targets).flatten()).mean()\n",
    "            assert actual_noise > 0.0\n",
    "        return np.array(noisy_targets).flatten(), actual_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4dd0d-3025-4dee-92fa-ec4d11017b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08df954-ebf8-452a-b9bb-f3b91f41efb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134d7f7-4c38-4f40-a5d7-bc84bd5d4510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e02b5d-ae8d-4c26-8eb7-273033313522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5f4fdd-aa31-4a2f-ba06-ab1c0081ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac0c77c-f6ff-414f-98ab-e8904984e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25242ca7-2aa5-4cec-9fbe-99473903983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2a9ee0-2bd3-4ba1-8ad3-e5fac95e3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/femnist/data/train/all_data_0_niid_0_keep_0_train_8.json', 'r') as f:\n",
    "    a = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acab355d-d6a0-448a-be56-79bbca397363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'num_samples', 'user_data'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a1900d8-855e-41f9-a127-da6d805e72fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a['users'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb98d7da-06a7-4a0f-8258-2d7aaa42fa8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f1307_25',\n",
       " 'f1129_22',\n",
       " 'f1379_47',\n",
       " 'f1096_05',\n",
       " 'f1204_09',\n",
       " 'f1432_44',\n",
       " 'f1172_25',\n",
       " 'f1382_07',\n",
       " 'f1021_16',\n",
       " 'f1011_35',\n",
       " 'f1151_39',\n",
       " 'f1475_14',\n",
       " 'f1329_29',\n",
       " 'f1054_20',\n",
       " 'f1181_27',\n",
       " 'f1317_00',\n",
       " 'f1193_46',\n",
       " 'f1091_00',\n",
       " 'f1213_10',\n",
       " 'f1345_41',\n",
       " 'f1489_03',\n",
       " 'f1395_14',\n",
       " 'f1133_00',\n",
       " 'f1200_36',\n",
       " 'f1274_24',\n",
       " 'f1315_48',\n",
       " 'f1263_18',\n",
       " 'f1272_09',\n",
       " 'f1340_02',\n",
       " 'f1184_21',\n",
       " 'f1197_00',\n",
       " 'f1310_44',\n",
       " 'f1439_27',\n",
       " 'f1012_30',\n",
       " 'f1290_49',\n",
       " 'f1000_45',\n",
       " 'f1123_09',\n",
       " 'f1296_02',\n",
       " 'f1248_28',\n",
       " 'f1454_12',\n",
       " 'f1169_08',\n",
       " 'f1220_42',\n",
       " 'f1426_27',\n",
       " 'f1034_02',\n",
       " 'f1470_34',\n",
       " 'f1455_14',\n",
       " 'f1482_39',\n",
       " 'f1314_29',\n",
       " 'f1100_46',\n",
       " 'f1417_14',\n",
       " 'f1481_35',\n",
       " 'f1140_23',\n",
       " 'f1049_26',\n",
       " 'f1342_17',\n",
       " 'f1023_49',\n",
       " 'f1234_26',\n",
       " 'f1237_18',\n",
       " 'f1113_16',\n",
       " 'f1479_40',\n",
       " 'f1498_08',\n",
       " 'f1074_34',\n",
       " 'f1242_30',\n",
       " 'f1041_18',\n",
       " 'f1486_35',\n",
       " 'f1457_23',\n",
       " 'f1198_41',\n",
       " 'f1176_40',\n",
       " 'f1300_41',\n",
       " 'f1288_23',\n",
       " 'f1024_46',\n",
       " 'f1354_22',\n",
       " 'f1112_30',\n",
       " 'f1341_45',\n",
       " 'f1152_03',\n",
       " 'f1466_48',\n",
       " 'f1293_37',\n",
       " 'f1171_06',\n",
       " 'f1159_29',\n",
       " 'f1499_12',\n",
       " 'f1325_20',\n",
       " 'f1434_10',\n",
       " 'f1097_20',\n",
       " 'f1459_30',\n",
       " 'f1271_05',\n",
       " 'f1057_34',\n",
       " 'f1026_41',\n",
       " 'f1331_49',\n",
       " 'f1050_39',\n",
       " 'f1384_42',\n",
       " 'f1269_14',\n",
       " 'f1450_34',\n",
       " 'f1190_46',\n",
       " 'f1119_27',\n",
       " 'f1186_07',\n",
       " 'f1065_10',\n",
       " 'f1335_30',\n",
       " 'f1082_22',\n",
       " 'f1240_01',\n",
       " 'f1275_19',\n",
       " 'f1179_01']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58cf4d8d-a41b-47b9-aa5c-e4f99ca7cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/femnist/data/test/all_data_0_niid_0_keep_0_test_8.json', 'r') as t:\n",
    "    b = json.load(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01a599d8-4e51-4005-96c6-399b48dd2f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'num_samples', 'user_data'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f4e523d-999f-4be3-9613-fa5cc0446681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b['users'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac3003b2-c26f-4f05-9c5c-1d435ffdc281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0f073d7-a38b-47ef-b7ee-280b58d61856",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.zeros(10)\n",
    "n = torch.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98e34a22-41a5-416f-964f-04d2b42d2c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.data, n.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "286cdfa3-16a8-44c7-b436-fdd371f9687f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1018,  1.1474,  0.6242, -0.3381,  0.2918, -1.0141, -0.8198, -0.3816,\n",
       "        -0.5045, -0.7239])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5959)\n",
    "torch.nn.init.normal_(m.data, 0.0, 1.0)\n",
    "\n",
    "torch.manual_seed(5252)\n",
    "torch.nn.init.normal_(n.data, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b32febe-7820-47ae-b057-21432baacf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.9307, -0.6656, -1.5053,  1.9334,  0.2129,  1.1681, -0.0685,  0.4326,\n",
       "         -1.0074,  0.2255]),\n",
       " tensor([ 1.1018,  1.1474,  0.6242, -0.3381,  0.2918, -1.0141, -0.8198, -0.3816,\n",
       "         -0.5045, -0.7239]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea3d08-5f04-4b1c-a630-d2ad7bf2c6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
